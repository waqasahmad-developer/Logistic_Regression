# üìò Logistic Regression Playground ‚Äî From Perceptron Trick to Gradient Descent

This repository offers an intuitive and hands-on approach to understanding **Logistic Regression**, starting from the basic perceptron rule to implementing a complete gradient descent optimization that mimics `scikit-learn`'s results. It's ideal for students and practitioners looking to deepen their understanding through interactive code.

---

## üöÄ What's Inside?

| Notebook | Highlights |
|----------|------------|
| **1Ô∏è‚É£ Logistic_Regression** | Classic perceptron rule using a hard step activation function. Visualizes how the decision boundary updates over iterations. |
| **2Ô∏è‚É£ Perceptron_Sigmoid** | Replaces the step function with a **sigmoid**, demonstrating its importance and how it enables gradient-based optimization. |
| **3Ô∏è‚É£ Logistic_Regression_GD** | Implements gradient descent on log-loss and compares the convergence and accuracy with `sklearn.linear_model.LogisticRegression`. |

---

## ‚ú® Key Features

- Visual explanation of why sigmoid is better than the step function for logistic regression  
- Comparison between hand-coded gradient descent and Scikit-learn's implementation  
- Real-time plotting of decision boundaries and loss curves  
- Clean, vectorized NumPy code for easy experimentation and modification 
